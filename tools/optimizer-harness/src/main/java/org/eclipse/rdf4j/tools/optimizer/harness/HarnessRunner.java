/*******************************************************************************
 * Copyright (c) 2025 Eclipse RDF4J contributors.
 *
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Eclipse Distribution License v1.0
 * which accompanies this distribution, and is available at
 * http://www.eclipse.org/org/documents/edl-v10.php.
 *
 * SPDX-License-Identifier: BSD-3-Clause
 *******************************************************************************/
// Some portions generated by Codex
package org.eclipse.rdf4j.tools.optimizer.harness;

import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.stream.Collectors;

import org.eclipse.rdf4j.model.Statement;
import org.eclipse.rdf4j.query.BindingSet;
import org.eclipse.rdf4j.query.QueryLanguage;
import org.eclipse.rdf4j.query.QueryResults;
import org.eclipse.rdf4j.query.TupleQuery;
import org.eclipse.rdf4j.query.TupleQueryResult;
import org.eclipse.rdf4j.query.algebra.QueryModelNode;
import org.eclipse.rdf4j.query.algebra.QueryRoot;
import org.eclipse.rdf4j.query.algebra.TupleExpr;
import org.eclipse.rdf4j.query.algebra.evaluation.impl.EvaluationStatistics;
import org.eclipse.rdf4j.query.algebra.evaluation.optimizer.CardinalityEstimator;
import org.eclipse.rdf4j.query.algebra.helpers.AbstractQueryModelVisitor;
import org.eclipse.rdf4j.query.explanation.Explanation;
import org.eclipse.rdf4j.query.explanation.GenericPlanNode;
import org.eclipse.rdf4j.repository.RepositoryConnection;
import org.eclipse.rdf4j.repository.sail.SailRepository;
import org.eclipse.rdf4j.sail.memory.MemoryStore;

public final class HarnessRunner {

	private static final String OPTIMIZER_FLAG = "rdf4j.optimizer.unionOptional.enabled";
	private static final String UNION_FLATTEN_FLAG = "rdf4j.optimizer.unionOptional.flatten.enabled";
	private static final String UNION_REORDER_FLAG = "rdf4j.optimizer.unionOptional.unionReorder.enabled";
	private static final String DUMP_PLANS_FLAG = "rdf4j.optimizer.unionOptional.dumpPlans";

	public static void main(String[] args) throws Exception {
		HarnessConfig config = HarnessConfig.fromArgs(args);
		Files.createDirectories(config.outputDir);

		List<Statement> statements = DatasetGenerator.generate(config);
		List<QueryCase> queries = QueryGenerator.generate(config);

		RunResult baseline = runProfile("baseline", false, statements, queries, config,
				config.outputDir.resolve("baseline.csv"));
		if (config.baselineOnly) {
			printSummary("baseline", baseline, config.outputDir.resolve("baseline-summary.txt"));
			return;
		}

		RunResult candidate = runProfile("candidate", true, statements, queries, config,
				config.outputDir.resolve("candidate.csv"));

		printSummary("baseline", baseline, config.outputDir.resolve("baseline-summary.txt"));
		printSummary("candidate", candidate, config.outputDir.resolve("candidate-summary.txt"));

		compareResults(baseline, candidate, config);
	}

	private static RunResult runProfile(String profileName, boolean enableOptimizers, List<Statement> statements,
			List<QueryCase> queries, HarnessConfig config, Path csvPath) throws Exception {
		PropertySnapshot snapshot = PropertySnapshot.capture(OPTIMIZER_FLAG, UNION_FLATTEN_FLAG, UNION_REORDER_FLAG,
				DUMP_PLANS_FLAG);
		applyOptimizerFlags(enableOptimizers);
		boolean dumpPlans = isDumpPlansEnabled();
		RunResult result = new RunResult(profileName);
		Path plansDir = config.outputDir.resolve("plans");
		Files.createDirectories(plansDir);

		SailRepository repository = new SailRepository(new MemoryStore());
		repository.init();
		try (RepositoryConnection connection = repository.getConnection()) {
			connection.add(statements);
			try (PlanMetricsWriter writer = new PlanMetricsWriter(csvPath)) {
				for (QueryCase queryCase : queries) {
					QueryOutcome outcome = executeQuery(connection, queryCase, writer, profileName, plansDir,
							config, dumpPlans);
					result.outcomes.put(queryCase.id, outcome);
					result.estimationErrors.addAll(outcome.estimationErrors);
				}
			}
		} finally {
			repository.shutDown();
			snapshot.restore();
		}
		return result;
	}

	private static QueryOutcome executeQuery(RepositoryConnection connection, QueryCase queryCase,
			PlanMetricsWriter writer, String runId, Path plansDir, HarnessConfig config, boolean dumpPlans)
			throws IOException {
		TupleQuery query = connection.prepareTupleQuery(QueryLanguage.SPARQL, queryCase.query);
		Map<String, Long> multiset;
		try (TupleQueryResult result = query.evaluate()) {
			List<BindingSet> bindings = QueryResults.asList(result);
			multiset = toMultiset(bindings);
		}

		Explanation explanation = query.explain(Explanation.Level.Timed);
		GenericPlanNode plan = explanation.toGenericPlanNode();
		Map<String, CardinalityEstimator.Estimate> estimates = estimateNodeIds(explanation);
		writer.writePlan(runId, queryCase.id, plan, estimates);

		double totalTimeMs = plan.getTotalTimeActual() == null ? 0.0 : plan.getTotalTimeActual();
		String planText = plan.toString();
		if (dumpPlans) {
			writePlanDumpForProfile(plansDir, queryCase.id, runId, planText);
		}
		List<EstimationError> errors = new ArrayList<>();
		collectEstimationErrors(queryCase.id, plan, "0", errors);

		return new QueryOutcome(multiset, totalTimeMs, planText, errors);
	}

	private static Map<String, CardinalityEstimator.Estimate> estimateNodeIds(Explanation explanation) {
		QueryModelNode root = extractQueryRoot(explanation);
		if (root == null) {
			return Map.of();
		}
		CardinalityEstimator estimator = new CardinalityEstimator(new EvaluationStatistics());
		Map<String, CardinalityEstimator.Estimate> estimates = new HashMap<>();
		collectEstimates(root, "0", estimator, estimates);
		return estimates;
	}

	private static QueryModelNode extractQueryRoot(Explanation explanation) {
		Object tupleExpr = explanation.tupleExpr();
		if (tupleExpr instanceof QueryRoot) {
			return ((QueryRoot) tupleExpr).getArg();
		}
		if (tupleExpr instanceof QueryModelNode) {
			return (QueryModelNode) tupleExpr;
		}
		return null;
	}

	private static void collectEstimates(QueryModelNode node, String nodeId, CardinalityEstimator estimator,
			Map<String, CardinalityEstimator.Estimate> estimates) {
		if (node instanceof TupleExpr) {
			estimates.put(nodeId, estimator.estimate((TupleExpr) node));
		}
		List<QueryModelNode> children = getChildren(node);
		for (int i = 0; i < children.size(); i++) {
			collectEstimates(children.get(i), nodeId + "." + i, estimator, estimates);
		}
	}

	private static List<QueryModelNode> getChildren(QueryModelNode node) {
		QueryModelChildrenVisitor visitor = new QueryModelChildrenVisitor();
		node.visitChildren(visitor);
		return visitor.getChildren();
	}

	private static final class QueryModelChildrenVisitor extends AbstractQueryModelVisitor<RuntimeException> {
		private List<QueryModelNode> children;

		@Override
		public void meetNode(QueryModelNode node) {
			if (children == null) {
				children = new ArrayList<>();
			}
			children.add(node);
		}

		List<QueryModelNode> getChildren() {
			return children == null ? Collections.emptyList() : children;
		}
	}

	private static void compareResults(RunResult baseline, RunResult candidate, HarnessConfig config)
			throws IOException {
		List<String> mismatches = new ArrayList<>();
		List<String> regressions = new ArrayList<>();

		for (Map.Entry<String, QueryOutcome> entry : baseline.outcomes.entrySet()) {
			String queryId = entry.getKey();
			QueryOutcome base = entry.getValue();
			QueryOutcome cand = candidate.outcomes.get(queryId);
			if (cand == null) {
				mismatches.add(queryId + ": candidate missing");
				continue;
			}
			if (!Objects.equals(base.multiset, cand.multiset)) {
				mismatches.add(queryId);
				writePlanDump(config.outputDir, queryId, base.planText, cand.planText);
				continue;
			}
			if (base.totalTimeMs > 0 && cand.totalTimeMs > base.totalTimeMs * (1.0 + config.maxRegression)) {
				regressions.add(queryId + " baseline=" + base.totalTimeMs + "ms candidate=" + cand.totalTimeMs + "ms");
				writePlanDump(config.outputDir, queryId, base.planText, cand.planText);
			}
		}

		if (!mismatches.isEmpty() || !regressions.isEmpty()) {
			StringBuilder message = new StringBuilder();
			if (!mismatches.isEmpty()) {
				message.append("Result mismatches: ")
						.append(String.join(", ", mismatches))
						.append(System.lineSeparator());
			}
			if (!regressions.isEmpty()) {
				message.append("Performance regressions: ")
						.append(String.join(", ", regressions))
						.append(System.lineSeparator());
			}
			throw new IllegalStateException(message.toString());
		}
	}

	private static void writePlanDump(Path outputDir, String queryId, String baselinePlan, String candidatePlan)
			throws IOException {
		Path plansDir = outputDir.resolve("plans");
		Files.createDirectories(plansDir);
		Path baselinePath = plansDir.resolve("query-" + queryId + "-baseline.txt");
		Path candidatePath = plansDir.resolve("query-" + queryId + "-candidate.txt");
		Files.writeString(baselinePath, baselinePlan, StandardCharsets.UTF_8);
		Files.writeString(candidatePath, candidatePlan, StandardCharsets.UTF_8);
	}

	private static void writePlanDumpForProfile(Path plansDir, String queryId, String profile, String planText)
			throws IOException {
		Path planPath = plansDir.resolve("query-" + queryId + "-" + profile + ".txt");
		Files.writeString(planPath, planText, StandardCharsets.UTF_8);
	}

	private static Map<String, Long> toMultiset(List<BindingSet> bindings) {
		Map<String, Long> counts = new HashMap<>();
		for (BindingSet bindingSet : bindings) {
			String key = bindingSetKey(bindingSet);
			counts.merge(key, 1L, Long::sum);
		}
		return counts;
	}

	private static String bindingSetKey(BindingSet bindingSet) {
		List<String> names = new ArrayList<>(bindingSet.getBindingNames());
		names.sort(String::compareTo);
		return names.stream()
				.map(name -> name + "=" + bindingSet.getValue(name))
				.collect(Collectors.joining("|"));
	}

	private static void collectEstimationErrors(String queryId, GenericPlanNode node, String nodeId,
			List<EstimationError> errors) {
		Double estimate = node.getResultSizeEstimate();
		Long actual = node.getResultSizeActual();
		if (estimate != null && actual != null && estimate > 0 && actual > 0) {
			double ratio = Math.max(actual / estimate, estimate / actual);
			errors.add(new EstimationError(queryId, nodeId, node.getType(), estimate, actual, ratio));
		}
		List<GenericPlanNode> children = node.getPlans();
		if (children == null) {
			return;
		}
		for (int i = 0; i < children.size(); i++) {
			collectEstimationErrors(queryId, children.get(i), nodeId + "." + i, errors);
		}
	}

	private static void printSummary(String profile, RunResult result, Path outputPath) throws IOException {
		List<EstimationError> topErrors = result.estimationErrors.stream()
				.sorted(Comparator.comparingDouble((EstimationError error) -> error.ratio).reversed())
				.limit(10)
				.collect(Collectors.toList());

		StringBuilder summary = new StringBuilder();
		summary.append(profile).append(" worst estimation errors").append(System.lineSeparator());
		for (EstimationError error : topErrors) {
			summary.append(String.format("query=%s node=%s op=%s ratio=%.2f estimate=%.2f actual=%d",
					error.queryId, error.nodeId, error.opType, error.ratio, error.estimate, error.actual))
					.append(System.lineSeparator());
		}
		Files.writeString(outputPath, summary.toString(), StandardCharsets.UTF_8);
	}

	private static void applyOptimizerFlags(boolean enabled) {
		String value = Boolean.toString(enabled);
		System.setProperty(OPTIMIZER_FLAG, value);
		System.setProperty(UNION_FLATTEN_FLAG, value);
		System.setProperty(UNION_REORDER_FLAG, value);
	}

	private static boolean isDumpPlansEnabled() {
		return Boolean.parseBoolean(System.getProperty(DUMP_PLANS_FLAG, "false"));
	}

	private static final class RunResult {
		private final String profile;
		private final Map<String, QueryOutcome> outcomes = new HashMap<>();
		private final List<EstimationError> estimationErrors = new ArrayList<>();

		private RunResult(String profile) {
			this.profile = profile;
		}
	}

	private static final class QueryOutcome {
		private final Map<String, Long> multiset;
		private final double totalTimeMs;
		private final String planText;
		private final List<EstimationError> estimationErrors;

		private QueryOutcome(Map<String, Long> multiset, double totalTimeMs, String planText,
				List<EstimationError> estimationErrors) {
			this.multiset = multiset;
			this.totalTimeMs = totalTimeMs;
			this.planText = planText;
			this.estimationErrors = estimationErrors;
		}
	}

	private static final class EstimationError {
		private final String queryId;
		private final String nodeId;
		private final String opType;
		private final double estimate;
		private final long actual;
		private final double ratio;

		private EstimationError(String queryId, String nodeId, String opType, double estimate, long actual,
				double ratio) {
			this.queryId = queryId;
			this.nodeId = nodeId;
			this.opType = opType;
			this.estimate = estimate;
			this.actual = actual;
			this.ratio = ratio;
		}
	}

	private static final class PropertySnapshot {
		private final Map<String, String> values;

		private PropertySnapshot(Map<String, String> values) {
			this.values = values;
		}

		private static PropertySnapshot capture(String... keys) {
			Map<String, String> snapshot = new HashMap<>();
			for (String key : keys) {
				snapshot.put(key, System.getProperty(key));
			}
			return new PropertySnapshot(snapshot);
		}

		private void restore() {
			for (Map.Entry<String, String> entry : values.entrySet()) {
				if (entry.getValue() == null) {
					System.clearProperty(entry.getKey());
				} else {
					System.setProperty(entry.getKey(), entry.getValue());
				}
			}
		}
	}
}
